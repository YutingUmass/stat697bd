{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVIBKaLDmvBu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import collections\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "from scipy.stats import mannwhitneyu\n",
        "from scipy.stats import wilcoxon\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT-d8tlqwAv5"
      },
      "source": [
        "# Apply $X^{2}$ test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaynsTACv2pg"
      },
      "source": [
        "def fdr_sample_qs(data, batch, sample_index, k = 30):\n",
        "    \n",
        "    \"\"\"\n",
        "    data= dataframe of gene experession data, rows are samples and columns are genes\n",
        "    batch = list of batch labels corresponding to the samples in the data\n",
        "    sample_index = the index of the sample that we want to find FDR of its neighborhood\n",
        "    k = k nearest neighbors\n",
        "    \"\"\"\n",
        "    \n",
        "    #conver dataframe to array\n",
        "    X = np.asarray(data)\n",
        "    \n",
        "    # find 50 first PCAs\n",
        "    \n",
        "    pca = PCA(n_components=50)\n",
        "    PCs = pca.fit_transform(X)\n",
        "    \n",
        "    # k nearest neighbors of each PC\n",
        "    \n",
        "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(PCs)\n",
        "    distances, indices = nbrs.kneighbors(PCs)\n",
        "    \n",
        "    #list of neighbors of sample s\n",
        "    nbrs_of_s = indices[sample_index] \n",
        "    \n",
        "    # what batches each neighbor of sample s belong to\n",
        "    batches_nbrs_s = [batch[index] for index in list(nbrs_of_s)]\n",
        "\n",
        "    # find frequency of each batch in nbrs_s\n",
        "    counter=collections.Counter(batches_nbrs_s)\n",
        "    freq_nbrs_s= list(counter.values())\n",
        "\n",
        "    counter=collections.Counter(batch)\n",
        "    n_samples = len(batch)\n",
        "    n_batch_1 = counter[1]\n",
        "    n_batch_2 = counter[2]\n",
        "\n",
        "    counter=collections.Counter(batch)\n",
        "    s, p = chisquare(f_obs = freq_nbrs_s,  f_exp =[k*n_batch_1/n_samples, k*n_batch_2/n_samples])\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3FReo2uwDSh"
      },
      "source": [
        "# Apply Mann-Whitney U test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uucIR0Bwv5Hx"
      },
      "source": [
        "def fdr_sample_mw(data, batch, sample_index, k):\n",
        "    \n",
        "    \"\"\"\n",
        "    data= dataframe of gene experession data, rows are samples and columns are genes\n",
        "    batch = list of batch labels corresponding to the samples in the data\n",
        "    sample_index = the index of the sample that we want to find FDR of its neighborhood\n",
        "    k = k nearest neighbors\n",
        "    \"\"\"\n",
        "        \n",
        "    # find 50 first PCAs\n",
        "    pca = PCA(n_components=50)\n",
        "    PCs = pca.fit_transform(data)\n",
        "    \n",
        "    # k nearest neighbors of each PC\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(PCs)\n",
        "    distances, indices = nbrs.kneighbors(PCs)\n",
        "    \n",
        "    #list of neighbors of sample s\n",
        "    nbrs_of_s = indices[sample_index] \n",
        "    \n",
        "    # what batches each neighbor of sample s belong to\n",
        "    batches_nbrs_s = [batch[index] for index in list(nbrs_of_s)]\n",
        "\n",
        "    counter=collections.Counter(batch)\n",
        "    n_samples = len(batch)\n",
        "    n_batch_1 = counter[1]\n",
        "    n_batch_2 = counter[2]\n",
        "    \n",
        "    # frequency of the overall dataset\n",
        "    freq = [n_batch_1/n_samples, n_batch_2/n_samples]\n",
        "    \n",
        "    # frequency of the overall dataset with the length k\n",
        "    expected_distr = [1 for i in range(round(k * freq[0]))] + [2 for i in range(round(k * freq[1]))]\n",
        "\n",
        "    s, p = mannwhitneyu(batches_nbrs_s,  expected_distr)\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSlAnymrwFax"
      },
      "source": [
        "# Apply Wilcoxon Signed-Rank test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhExag4Bv77e"
      },
      "source": [
        "def fdr_sample_w(data, batch, sample_index, k):\n",
        "    \n",
        "    \"\"\"\n",
        "    data= dataframe of gene experession data, rows are samples and columns are genes\n",
        "    batch = list of batch labels corresponding to the samples in the data\n",
        "    sample_index = the index of the sample that we want to find FDR of its neighborhood\n",
        "    k = k nearest neighbors\n",
        "    \"\"\"\n",
        "        \n",
        "    # find 50 first PCAs\n",
        "    pca = PCA(n_components=50)\n",
        "    PCs = pca.fit_transform(data)\n",
        "    \n",
        "    # k nearest neighbors of each PC\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(PCs)\n",
        "    distances, indices = nbrs.kneighbors(PCs)\n",
        "    \n",
        "    #list of neighbors of sample s\n",
        "    nbrs_of_s = indices[sample_index] \n",
        "    \n",
        "    # what batches each neighbor of sample s belong to\n",
        "    batches_nbrs_s = [batch[index] for index in list(nbrs_of_s)]\n",
        "\n",
        "    counter=collections.Counter(batch)\n",
        "    n_samples = len(batch)\n",
        "    n_batch_1 = counter[1]\n",
        "    n_batch_2 = counter[2]\n",
        "    \n",
        "    # frequency of the overall dataset\n",
        "    freq = [n_batch_1/n_samples, n_batch_2/n_samples]\n",
        "    \n",
        "    # frequency of the overall dataset with the length k\n",
        "    expected_distr = [1 for i in range(round(k * freq[0]))] + [2 for i in range(round(k * freq[1]))]\n",
        "\n",
        "    s, p = wilcoxon(batches_nbrs_s,  expected_distr)\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsQ_r6SwHwh"
      },
      "source": [
        "# Kolmogorov-Smirnov's test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc1PisYhv978"
      },
      "source": [
        "def fdr_sample_ks(data, batch, sample_index, k = 20):\n",
        "    \n",
        "    \"\"\"\n",
        "    data= data as array of gene experession data, rows are samples and columns are genes\n",
        "    batch = list of batch labels corresponding to the samples in the data\n",
        "    sample_index = the index of the sample that we want to find FDR of its neighborhood\n",
        "    k = k nearest neighbors\n",
        "    \"\"\"\n",
        "        \n",
        "    # find 50 first PCAs\n",
        "    pca = PCA(n_components=50)\n",
        "    PCs = pca.fit_transform(data)\n",
        "    \n",
        "    # k nearest neighbors of each PC\n",
        "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(PCs)\n",
        "    distances, indices = nbrs.kneighbors(PCs)\n",
        "    \n",
        "    #list of neighbors of sample s\n",
        "    nbrs_of_s = indices[sample_index] \n",
        "        \n",
        "    # what batches each neighbor of sample s belong to\n",
        "    batches_nbrs_s = [batch[index] for index in list(nbrs_of_s)]\n",
        "\n",
        "    counter=collections.Counter(batch)\n",
        "    n_samples = len(batch)\n",
        "    n_batch_1 = counter[1]\n",
        "    n_batch_2 = counter[2]\n",
        "    \n",
        "    # frequency of the overall dataset\n",
        "    freq = [n_batch_1/n_samples, n_batch_2/n_samples]\n",
        "    \n",
        "    # frequency of the overall dataset with the length k\n",
        "    expected_distr = [1 for i in range(round(k * freq[0]))] + [2 for i in range(round(k * freq[1]))]\n",
        "\n",
        "    s, p = ks_2samp(batches_nbrs_s,  expected_distr)\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG-HlGXofyVi"
      },
      "source": [
        "# Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_yujeWOgVCT"
      },
      "source": [
        "def simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k = 20):\n",
        "  data_weak = pd.read_csv(dir_weak) \n",
        "  data_weak = data_weak.rename(columns={ data_weak.columns[0]: \"Genes\" })\n",
        "  data_weak = data_weak.set_index(\"Genes\")\n",
        "  data_weak = data_weak.transpose()\n",
        "\n",
        "  data_mild = pd.read_csv(dir_mild) \n",
        "  data_mild = data_mild.rename(columns={ data_mild.columns[0]: \"Genes\" })\n",
        "  data_mild = data_mild.set_index(\"Genes\")\n",
        "  data_mild = data_mild.transpose()  \n",
        "\n",
        "  data_strong = pd.read_csv(dir_strong) \n",
        "  data_strong = data_strong.rename(columns={data_strong.columns[0]: \"Genes\" })\n",
        "  data_strong = data_strong.set_index(\"Genes\")\n",
        "  data_strong = data_strong.transpose()\n",
        "\n",
        "  batch_1 = [1 for i in range(n_batch1)]\n",
        "  batch_2 = [2 for i in range(n_batch2)]\n",
        "\n",
        "  #Assign the first n_batch1 samples to batch 1 and the next n_batch2 samples to batch 2\n",
        "  batch = batch_1 + batch_2\n",
        "  n_samples = len(batch)\n",
        "\n",
        "  ##### Chi-square test #####\n",
        "\n",
        "  # false discovery rate\n",
        "  FDR_CHI_SQU ={}\n",
        "\n",
        "  # rejection rate\n",
        "  RR_CHI = {}\n",
        "\n",
        "  # list of p-value\n",
        "  p_CHI_weak = [fdr_sample_qs(data_weak, batch, i, k) for i in range(n_samples)]\n",
        "  p_CHI_mild = [fdr_sample_qs(data_mild, batch, i, k) for i in range(n_samples)]\n",
        "  p_CHI_strong = [fdr_sample_qs(data_strong, batch, i, k) for i in range(n_samples)]\n",
        "\n",
        "  # fdr\n",
        "  FDR_CHI_SQU['weak'] = 1-np.mean(p_CHI_weak)\n",
        "  FDR_CHI_SQU['mild'] = 1-np.mean(p_CHI_mild)\n",
        "  FDR_CHI_SQU['strong'] = 1-np.mean(p_CHI_strong)\n",
        "  print(\"FDR_CHI_SQU: \", FDR_CHI_SQU)\n",
        "\n",
        "  # rejection rate\n",
        "  RR_CHI_SQU['weak'] = sum(1 for i in p_CHI_weak if i < alpha)/len(p_CHI_weak)\n",
        "  RR_CHI_SQU['mild'] = sum(1 for i in p_CHI_mild if i < alpha)/len(p_CHI_mild)\n",
        "  RR_CHI_SQU['strong'] = sum(1 for i in p_CHI_strong if i < alpha)/len(p_CHI_strong)\n",
        "  print(\"RR_CHI_SQU: \", RR_CHI_SQU)\n",
        "\n",
        "\n",
        "  ##### Mann-Whitney U test #####\n",
        "  FDR_M_W = {}  \n",
        "  RR_M_W = {} \n",
        "\n",
        "  # list of p-value\n",
        "  p_M_W_weak = [fdr_sample_mw(data_weak, batch, i, k) for i in range(n_samples)]\n",
        "  p_M_W_mild = [fdr_sample_mw(data_mild, batch, i, k) for i in range(n_samples)]\n",
        "  p_M_W_strong = [fdr_sample_mw(data_strong, batch, i, k) for i in range(n_samples)]\n",
        "\n",
        "  # fdr\n",
        "  FDR_M_W['weak'] = 1-np.mean(p_M_W_weak)\n",
        "  FDR_M_W['mild'] = 1-np.mean(p_M_W_mild)\n",
        "  FDR_M_W['strong'] = 1-np.mean(p_M_W_strong)\n",
        "  print(\"FDR_M_W: \", FDR_M_W)\n",
        "\n",
        "  # rejection rate\n",
        "  RR_M_W['weak'] = sum(1 for i in p_M_W_weak if i < alpha)/len(p_M_W_weak)\n",
        "  RR_M_W['mild'] = sum(1 for i in p_M_W_mild if i < alpha)/len(p_M_W_mild)\n",
        "  RR_M_W['strong'] = sum(1 for i in p_M_W_strong if i < alpha)/len(p_M_W_strong)\n",
        "  print(\"RR_M_W: \", RR_M_W)\n",
        "\n",
        "\n",
        "  ##### Wilcoxon Signed-Rank test #####\n",
        "  FDR_W = {} \n",
        "  RR_W = {}  \n",
        "\n",
        "  # list of p-value\n",
        "  p_W_weak = [fdr_sample_w(data_weak, batch, i, k) for i in range(n_samples)]\n",
        "  p_W_mild = [fdr_sample_w(data_mild, batch, i, k) for i in range(n_samples)]\n",
        "  p_W_strong = [fdr_sample_w(data_strong, batch, i, k) for i in range(n_samples)]\n",
        "\n",
        "  # fdr\n",
        "  FDR_W['weak'] = 1-np.mean(p_W_weak)\n",
        "  FDR_W['mild'] = 1-np.mean(p_W_mild)\n",
        "  FDR_W['strong'] = 1-np.mean(p_W_strong)\n",
        "  print(\"FDR_W: \", FDR_W)\n",
        "\n",
        "  # rejection rate\n",
        "  RR_W['weak'] = sum(1 for i in p_W_weak if i < alpha)/len(p_W_weak)\n",
        "  RR_W['mild'] = sum(1 for i in p_W_mild if i < alpha)/len(p_W_mild)\n",
        "  RR_W['strong'] = sum(1 for i in p_W_strong if i < alpha)/len(p_W_strong)\n",
        "  print(\"RR_W: \", RR_W)\n",
        "\n",
        " \n",
        "  ##### Kolmogorov-Smirnov's test #####\n",
        "  FDR_K_S = {} \n",
        "  RR_K_S = {}  \n",
        "\n",
        "  # list of p-value\n",
        "  p_K_S_weak = [fdr_sample_ks(data_weak, batch, i, k) for i in range(n_samples)]\n",
        "  p_K_S_mild = [fdr_sample_ks(data_mild, batch, i, k) for i in range(n_samples)]\n",
        "  p_K_S_strong = [fdr_sample_ks(data_strong, batch, i, k) for i in range(n_samples)]\n",
        "\n",
        "  # fdr\n",
        "  FDR_K_S['weak'] = 1-np.mean(p_K_S_weak)\n",
        "  FDR_K_S['mild'] = 1-np.mean(p_K_S_mild)\n",
        "  FDR_K_S['strong'] = 1-np.mean(p_K_S_strong)\n",
        "  print(\"FDR_K_S: \", FDR_K_S)\n",
        "\n",
        "  # rejection rate\n",
        "  RR_K_S['weak'] = sum(1 for i in p_K_S_weak if i < alpha)/len(p_K_S_weak)\n",
        "  RR_K_S['mild'] = sum(1 for i in p_K_S_mild if i < alpha)/len(p_K_S_mild)\n",
        "  RR_K_S['strong'] = sum(1 for i in p_K_S_strong if i < alpha)/len(p_K_S_strong)\n",
        "  print(\"RR_K_S: \", RR_K_S)\n",
        "\n",
        "  return   FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxuQO7Ttqkn6"
      },
      "source": [
        "def merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S):\n",
        "  ds_fdr = [FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S]\n",
        "  ds_rr = [RR_CHI_SQU, RR_M_W, RR_W, RR_K_S]\n",
        "  d_fdr = {}\n",
        "  ds_rr = {}\n",
        "\n",
        "  for k in FDR_CHI_SQU.keys():\n",
        "    d_fdr[k] = tuple(d_fdr[k] for d in ds_fdr)\n",
        "    d_rr[k] = tuple(d_rr[k] for d in ds_rr)\n",
        "\n",
        "  # convert dictionary to panda dataframe\n",
        "  FDR_result = pd.DataFrame.from_dict(d_fdr, orient='index',\n",
        "                       columns=['FDR_CHI', 'FDR_M_W', 'FDR_W', 'FDR_K_S'])\n",
        "  RR_result = pd.DataFrame.from_dict(d_rr, orient='index',\n",
        "                       columns=['RR_CHI', 'RR_M_W', 'RR_W', 'RR_K_S'])\n",
        "  return FDR_result, RR_result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrEyeHr8mJMo"
      },
      "source": [
        "# Simulation 1:1 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-cLyrpXmFPi"
      },
      "source": [
        "n_batch_1 = 250\n",
        "n_batch_2 = 250\n",
        "\n",
        "dir_weak = 'weak/df.sim.weak.1.1.csv'\n",
        "dir_mild = 'mild/df.sim.mild.1.1.csv'\n",
        "dir_strong = 'strong/df.sim.strong.1.1.csv'\n",
        "FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S = simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k =20)\n",
        "FDR_result, RR_result = merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S)\n",
        "FDR_result.to_csv('FDR_result_ratio_1_1.csv', index=True)\n",
        "RR_result.to_csv('RR_result_ratio_1_1.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMj1isBNvO5r"
      },
      "source": [
        "# Simulation 1:3 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5GRoSswvPEB"
      },
      "source": [
        "n_batch_1 = 500/4\n",
        "n_batch_2 = 500*(3/4)\n",
        "\n",
        "dir_weak = 'weak/df.sim.weak.1.3.csv'\n",
        "dir_mild = 'mild/df.sim.mild.1.3.csv'\n",
        "dir_strong = 'strong/df.sim.strong.1.3.csv'\n",
        "FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S = simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k =20)\n",
        "FDR_result, RR_result = merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S)\n",
        "FDR_result.to_csv('FDR_result_ratio_1_3.csv', index=True)\n",
        "RR_result.to_csv('RR_result_ratio_1_3.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2li0b08vSio"
      },
      "source": [
        "# Simulation 1:4 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJYgC6jzvkG4"
      },
      "source": [
        "n_batch_1 = 500/5\n",
        "n_batch_2 = 500*(4/5)\n",
        "\n",
        "dir_weak = 'weak/df.sim.weak.1.4.csv'\n",
        "dir_mild = 'mild/df.sim.mild.1.4.csv'\n",
        "dir_strong = 'strong/df.sim.strong.1.4.csv'\n",
        "FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S = simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k =20)\n",
        "FDR_result, RR_result = merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S)\n",
        "FDR_result.to_csv('FDR_result_ratio_1_4.csv', index=True)\n",
        "RR_result.to_csv('RR_result_ratio_1_4.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6KCTlIvSqm"
      },
      "source": [
        "# Simulation 1:9 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-QYWvLvklX"
      },
      "source": [
        "n_batch_1 = 500/10\n",
        "n_batch_2 = 500*(9/10)\n",
        "\n",
        "dir_weak = 'weak/df.sim.weak.1.9.csv'\n",
        "dir_mild = 'mild/df.sim.mild.1.9.csv'\n",
        "dir_strong = 'strong/df.sim.strong.1.9.csv'\n",
        "FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S = simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k =20)\n",
        "FDR_result, RR_result = merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S)\n",
        "FDR_result.to_csv('FDR_result_ratio_1_9.csv', index=True)\n",
        "RR_result.to_csv('RR_result_ratio_1_9.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzDnUA5kvSte"
      },
      "source": [
        "# Simulation 1:19 ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21fb4QKXvl_o"
      },
      "source": [
        "n_batch_1 = 500/20\n",
        "n_batch_2 = 500*(19/20)\n",
        "\n",
        "dir_weak = 'weak/df.sim.weak.1.19.csv'\n",
        "dir_mild = 'mild/df.sim.mild.1.19.csv'\n",
        "dir_strong = 'strong/df.sim.strong.1.19.csv'\n",
        "FDR_CHI_SQU, RR_CHI,  FDR_M_W, RR_M_W, FDR_W, RR_W, FDR_K_S, RR_K_S = simulation(dir_weak, dir_mild, dir_strong, n_batch_1, n_batch_2, alpha = 0.05, k =20)\n",
        "FDR_result, RR_result = merge_dict(FDR_CHI_SQU, FDR_M_W, FDR_W, FDR_K_S, RR_CHI_SQU, RR_M_W, RR_W, RR_K_S)\n",
        "FDR_result.to_csv('FDR_result_ratio_1_19.csv', index=True)\n",
        "RR_result.to_csv('RR_result_ratio_1_19.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}